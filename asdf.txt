# Texto:
# Proyecto 1 | Inteligencia Artificial
## Integrantes:
- **César Jiménez Salazar** - 2021052709
- **Maynor Martínez Hernández** - 2021052792
- **Fernanda Murillo Mena** - 2021077803

## Descripción
Este proyecto tiene como objetivo principal aplicar diversas técnicas de clasificación de datos aplicados para dos conjuntos de datos, esto permite explorar diversas herramientas relacionadas al Machine Learning, y contribuir al desarrollo del conocimiento a partir de la investigación. Se utilizarán los modelos KNN, regresión lineal y redes neuronales con el fin de analizar ambos conjuntos de datos.

## Conjunto de datos
1. **Pima Indians Diabetes Database**
    - **URL:** https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database
    - **Descripción:** Este conjunto de datos proviene del Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales. El objetivo del conjunto de datos es predecir de forma diagnóstica si un paciente tiene diabetes o no, basándose en ciertas mediciones de diagnóstico incluidas en el conjunto de datos. Se impusieron varias restricciones a la selección de estas instancias de una base de datos más grande. En particular, todos los pacientes aquí son mujeres de al menos 21 años de edad de ascendencia india Pima.
2. **Heart Failure Prediction Dataset** <br>
   Este fue el dataset elegido por el equipo de trabajo debido a que fue el dataset relacionado a salud con una cantidad de features adecuada y el más parecido al primer dataset dado por el profesor en cuanto al tamaño de samples. 
    - **URL:** https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction
    - **Descripción:** Las enfermedades cardiovasculares (ECV) son la principal causa de muerte a nivel mundial y se estima que cobran 17,9 millones de vidas cada año, lo que representa el 31% de todas las muertes en todo el mundo. Cuatro de cada cinco muertes por ECV se deben a ataques cardíacos y accidentes cerebrovasculares, y un tercio de estas muertes ocurren prematuramente en mujeres menores de 70 años. La insuficiencia cardíaca es un evento común causado por enfermedades cardiovasculares y este conjunto de datos contiene 11 características que pueden usarse para predecir una posible enfermedad cardíaca. Las mujeres con enfermedades cardiovasculares o que tienen un alto riesgo cardiovascular (debido a la presencia de uno o más factores de riesgo como hipertensión, diabetes, hiperlipidemia o enfermedades ya establecidas) necesitan una detección y un tratamiento precoz en el que un modelo de aprendizaje automático puede ser de gran ayuda.

# Texto:
## Pima Indians Diabetes Database
Inicialmente, se procede con la exploración y el preprocesamiento de datos para el conjunto de datos *Pima Indians Diabetes Database*. Dicho dataset fue dado por el profesor del curso.

### Analisis Exploratorio de Datos
Primero, se importan las librerías necesarias para el análisis exploratorio de datos.

# Código:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Texto:
Ahora, se procede a cargar los datos del dataset. Del mismo modo, se comienza a explorar el conjunto de datos para obtener la información básica importante. Para lograr esto, se visualizan algunos datos para poder ver las columnas y los datos correspondientes, se obtienen las estadísticas básicas, la cantidad de datos faltantes y la correlación entre columnas.

# Código:
# Load dataset
df_diabetes = pd.read_csv('diabetes.csv')

# Display 20 rows 
print(df_diabetes.head(20))

# Código:
# Get basic statistics 
print(df_diabetes.describe())

# Código:
# Number of missing values in each column
print(df_diabetes.isnull().sum())

# Código:
# Correlation between columns
print(df_diabetes.corr())

# Código:
df_diabetes.count()

# Texto:
Con esta información, se obtiene que el dataset posee 9 columnas de las cuales *pregnancies* tiene la posibilidad de tener un 0 sin que sea un problema para el análisis, esto porque una mujer puede tener 0 embarazos. Además, la columna *outcome* tiene un valor binario por lo que también puede tener un valor 0. Se evidencia que el dataset tiene una cantidad de 768 datos y que no hay datos nulos o faltantes.



# Texto:
#### **Eliminar filas duplicadas**


Ahora, se procede a revisar si hay filas duplicadas para eliminarlas. Del mismo modo, tras el análisis anterior se pudo notar que algunas columnas (además de las permitidas) poseen valores en 0 por lo que es de suma importancia corregirlo para el análisis del dataset con los modelos más adelante. 


# Código:
# Check for duplicate rows and remove them
print(df_diabetes.duplicated().sum())
df_diabetes = df_diabetes.drop_duplicates()

# Texto:
#### **Imputación de datos**
Una técnica utilizada para tratar con valores faltantes en un conjunto de datos es la **_imputación de datos_**. El objetivo de la imputación es reemplazar los datos faltantes con valores sustitutos para permitir el análisis sin descartar *features* por la falta de algunos valores. Usar la media como valor de imputación es apropiado porque representa el punto central de la distribución y minimiza la distorsión.

Por este motivo, se decidió recurrir a la *imputación de datos por la media*, es decir, reemplazar los valores en 0 encontrados en todas las columnas excepto *pregnancies* y *outcome* por la media correspondiente a la columna.

# Código:
# Check for missing values for 0 in columns except for the 'Pregnancies' column and 'Outcome' column and replace them with the mean of the respective column
df_diabetes['Glucose'] = df_diabetes['Glucose'].replace(0, np.nan)
df_diabetes['BloodPressure'] = df_diabetes['BloodPressure'].replace(0, np.nan)
df_diabetes['SkinThickness'] = df_diabetes['SkinThickness'].replace(0, np.nan)
df_diabetes['Insulin'] = df_diabetes['Insulin'].replace(0, np.nan)
df_diabetes['BMI'] = df_diabetes['BMI'].replace(0, np.nan)
df_diabetes['DiabetesPedigreeFunction'] = df_diabetes['DiabetesPedigreeFunction'].replace(0, np.nan)
df_diabetes['Age'] = df_diabetes['Age'].replace(0, np.nan)
df_diabetes.fillna(df_diabetes.mean(), inplace=True)

# Texto:
- Debido a que no habían filas duplicadas, el resultado fue cero. 
- Tras el cambio del dataset realizado con la imputación por la media, se puede proceder a analizar mediante el uso de diversos diagramas para ver el comportamiento del dataset de manera visual.

El siguiente histograma permitirá ver el rango de edades que se encuentran dentro del conjunto de datos, esto con el fin de obtener una idea general de los datos obtenidos.

# Código:
# Histogram of the Age column
plt.hist(df_diabetes['Age'], bins=10, color='#3CB371', edgecolor='black')
plt.title('Histogram of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Texto:
Con la información obtenida se evidencia que la mayoría de mujeres se encuentra en un rango de edad entre 20 y 30 años. 

# Texto:
#### **Visualización de los datos**
Ahora, se procede con un análisis un poco más profundo. Se desea observar el porcentaje y la cantidad de mujeres diabéticas en comparación con las mujeres que no lo padecen. Dónde se provará diferentes casos para el dataset:
1. Balanceo
2. Gráficos de Barra
3. Gráficos de dispersion 

# Texto:
- **Gráficos para mostrar el Balanceo de Datos**

# Código:
# Distribution of the Outcome column for people with diabetes and people without diabetes
palette_colors = {0: "#40E0D0", 1: "#006064"}  
sns.countplot(x='Outcome', data=df_diabetes, hue="Outcome", palette=palette_colors)
plt.title('Distribution of Outcome')
plt.xlabel('Outcome')
plt.ylabel('Frequency')
plt.xticks([0, 1], ['Non-Diabetic', 'Diabetic'])
plt.show()

# Pie chart to visualize the proportion of diabetic and non-diabetic individuals
labels = ['Non-Diabetic', 'Diabetic']
sizes = df_diabetes['Outcome'].value_counts()
colors = ['#40E0D0', '#006064']
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Proportion of Diabetic and Non-Diabetic Individuals')
plt.axis('equal')
plt.show()

# Texto:
Con base en los resultados obtenidos, se puede ver que existe una mayor cantidad de mujeres que no padecen de diabetes en el conjunto de datos con un 65.1% de los 768 datos existentes. A demás, es importante ver que en el *pie chart* el dataset no está balanceado.

# Texto:
- **Gráficos de barra**

Para obtener una idea de como están relacionados los *features* con el hecho de que una mujer sea diabética o no, se pretende comparar cada columna con la columna de *outcome*. Para agilizar este proceso, primero se creó una función que recibe el dataset, el arreglo de *features* con la cual se pretende comparar y la columna con la que se va a comparar.

# Código:
def plot_stacked_grid_histogram(df, features, split_column):
    num_features = len(features)
    colors = ['#40E0D0', '#006064']
    
    num_rows = (num_features + 2) // 3  
    
    fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows)) 

    if num_rows > 1:
        axes = axes.flatten()
    
    for i, feature in enumerate(features):
        ax = axes[i] if num_rows > 1 else axes
        if feature in df.columns and split_column in df.columns:
            ax.hist([df[df[split_column] == 0][feature], df[df[split_column] == 1][feature]], 
                    bins=10, stacked=True, color=colors, 
                    label=[f'Not {split_column}', f'{split_column}'], edgecolor='black')
            ax.set_title(f'{feature} by {split_column}')
            ax.set_xlabel(feature)
            ax.set_ylabel('Count')
            ax.legend()
        else:
            print(f"The feature '{feature}' or the split column '{split_column}' is not in the DataFrame.")
    for i in range(num_features, num_rows * 3):
        fig.delaxes(axes.flatten()[i])

    plt.tight_layout()  
    plt.show()

# Código:
# Call the function to plot the stacked histograms
plot_stacked_grid_histogram(df_diabetes, ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'], 'Outcome')

# Texto:
Con base en los histogramas obtenidos, es fácil de observar que en su mayoría para cualquier feature donde están los valores altos es donde hay más concentración de mujeres diabéticas. Es importante ver que no es tan significativo en algunos casos como lo es la presión sanguínea.

# Texto:
- **Gráficos de dispersión**

Ahora, para observar que tan relacionada está la edad con estos datos vistos anteriormente, se pretende evaluar de manera similar cada columna en comparación de la edad para ver las diabéticas y las no diabéticas. Para este caso se recurrió al uso de diagramas de dispersión. Se seguió una técnica para agilizar el proceso similar al anterior donde se crea una función que luego se llama con el nombre del *dataset*, el arreglo de *features* a evaluar, las columnas a comparar y los *labels*.


# Código:
def scatter_plots_grid_by_feature_and_split(df, features, split_column, y_column, labels):
    num_features = len(features)
    colors = ['#40E0D0', '#006064']

    num_rows = (num_features + 2) // 3  

    fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows))  
    axes_flattened = axes.flatten() if num_rows > 1 else [axes]

    for i, feature in enumerate(features):
        ax = axes_flattened[i]
        for idx, outcome in enumerate(sorted(df[split_column].unique())):
            subset = df[df[split_column] == outcome]
            ax.scatter(subset[feature], subset[y_column], c=colors[idx], label=labels[idx], alpha=0.6)

        ax.set_title(f'{feature} vs {y_column}')
        ax.set_xlabel(feature)
        ax.set_ylabel(y_column)
        if i == 0:  
            ax.legend()

    for i in range(num_features, num_rows * 3):
        fig.delaxes(axes_flattened[i])

    plt.tight_layout()  
    plt.show()

# Código:
labels = ['Non-Diabetic', 'Diabetic']
scatter_plots_grid_by_feature_and_split(df_diabetes, ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction'], 'Outcome', 'Age', labels)


# Texto:
Con los resultados obtenidos, se muestra que las mujeresd diabéticas tienden a estar entre los valores más bajos en cuantos a edad. Los datos están más concentrados en las edades bajas en todos los *features* sólo en algunos casos como los embarazos o nuevamente la prsión sanguíonea tienen más disperción de datos.

# Texto:
#### **Remover outliers**

Los outliers son valores que difieren significativamente del resto de los datos. Pueden ser el resultado de errores de medición, variabilidad en la medición o incluso variaciones naturales en la población. Estos podrían ser valores extremadamente altos o bajos en las características como la glucosa, presión arterial, grosor de la piel, entre otros, que no se alinean con el rango esperado de estos parámetros.

# Código:
from scipy import stats
import numpy as np
# Calculate Z-scores for each numerical feature in the DataFrame
z_scores = np.abs(stats.zscore(df_diabetes.select_dtypes(include=[np.number])))

# Set a threshold for identifying outliers, typically 3
threshold = 3

# Create a boolean DataFrame where Z-scores exceed the threshold
outliers = (z_scores > threshold)

# Filter out the outliers from the DataFrame to create a new one without outliers
df_no_outliers = df_diabetes[~(outliers).any(axis=1)]

print(f"Original DataFrame size: {df_diabetes.shape}")
print(f"DataFrame size without outliers: {df_no_outliers.shape}")

# Set up the matplotlib figure
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), sharey=True)

# Titles for the plots
titles = ['Original Dataset', 'Dataset Without Outliers']

# DataFrames for plotting
dataframes = [df_diabetes, df_no_outliers]

# Colors for the plots
colors_original = ['#40E0D0', '#006064']  # Non-Diabetic and Diabetic for the original dataset
colors_no_outliers = ['red', 'darkred']  # Non-Diabetic and Diabetic for the dataset without outliers

for ax, df, title, color_set in zip(axes, dataframes, titles, [colors_original, colors_no_outliers]):
    # Scatter plot for Non-Diabetic
    ax.scatter(df[df['Outcome'] == 0]['Glucose'], df[df['Outcome'] == 0]['Age'], alpha=0.6, label='Non-Diabetic', color=color_set[0])
    # Scatter plot for Diabetic
    ax.scatter(df[df['Outcome'] == 1]['Glucose'], df[df['Outcome'] == 1]['Age'], alpha=0.6, label='Diabetic', color=color_set[1])
    ax.set_title(title)
    ax.set_xlabel('Glucose')
    ax.set_ylabel('Age')
    ax.legend()

plt.tight_layout()
plt.show()


# Texto:
Observando los cambios anteriores se puede evidenciar que: 
- <div style="display: inline-block; width: 12px; height: 12px; background-color: #046266; border: 1px solid black;"></div> dataset original, presenta mayor dispersión de datos en comparación al <div style="display: inline-block; width: 12px; height: 12px; background-color: #CC5184; border: 1px solid black;"></div> nuevo dataset (sin outliers), por lo que esto nos proporciana diferentes datas para realizar pruebas.


# Texto:
### Preprocesamiento de datos
En esta sección se procede a realizar el preprocesamiento de datos para el dataset *Pima Indians Diabetes Database*. Que consiste en separar el set de datos en training (80%) y testing (20%).

# Código:
random_state_result = 80

# Texto:
## Original Dataset


# Código:

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Split the data into features and target variable
X = df_diabetes.drop('Outcome', axis=1)
y = df_diabetes['Outcome']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state_result)


# Train the KNN model with the original dataset without scaling
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Predict on the test set
y_pred_original = knn.predict(X_test)

# Calculate and print metrics
print("Original Dataset - Without Scaling")
print(f"Accuracy: {accuracy_score(y_test, y_pred_original)}")
print(f"Precision: {precision_score(y_test, y_pred_original)}")
print(f"Recall: {recall_score(y_test, y_pred_original)}")


# Código:

# Normalize the features
scaler_norm = MinMaxScaler()
X_train_normalized = scaler_norm.fit_transform(X_train)
X_test_normalized = scaler_norm.transform(X_test)

# Train the KNN model
knn = KNeighborsClassifier()
knn.fit(X_train_normalized, y_train)

# Predict on the test set
y_pred_norm = knn.predict(X_test_normalized)

# Calculate and print the metrics
print("Original Dataset - Normalization")
print(f"Accuracy: {accuracy_score(y_test, y_pred_norm)}")
print(f"Precision: {precision_score(y_test, y_pred_norm)}")
print(f"Recall: {recall_score(y_test, y_pred_norm)}")


# Código:
from sklearn.preprocessing import StandardScaler

# Standardize the features
scaler_std = StandardScaler()
X_train_standardized = scaler_std.fit_transform(X_train)
X_test_standardized = scaler_std.transform(X_test)

# Train the KNN model
knn.fit(X_train_standardized, y_train)
y_pred_std = knn.predict(X_test_standardized)

# Calculate and print the metrics
print("Original Dataset - Standardization")
print(f"Accuracy: {accuracy_score(y_test, y_pred_std)}")
print(f"Precision: {precision_score(y_test, y_pred_std)}")
print(f"Recall: {recall_score(y_test, y_pred_std)}")


# Texto:
## Orginal datase without the pregnancies column (normalization and standarization).

# Código:
# Drop the 'Pregnancies' column
X_no_preg = df_diabetes.drop(['Outcome', 'Pregnancies'], axis=1)
X_train_no_preg, X_test_no_preg, y_train, y_test = train_test_split(X_no_preg, y, test_size=0.2, random_state=random_state_result)
knn.fit(X_train_no_preg, y_train)
y_pred_no_preg = knn.predict(X_test_no_preg)

# Calculate and print the metrics
print("Original Dataset without Pregnancies Column - Without Scaling")
print(f"Accuracy: {accuracy_score(y_test, y_pred_no_preg)}")
print(f"Precision: {precision_score(y_test, y_pred_no_preg)}")
print(f"Recall: {recall_score(y_test, y_pred_no_preg)}")


# Código:

# Normalize the features
X_train_no_preg_normalized = scaler_norm.fit_transform(X_train_no_preg)
X_test_no_preg_normalized = scaler_norm.transform(X_test_no_preg)
knn.fit(X_train_no_preg_normalized, y_train)
y_pred_no_preg_norm = knn.predict(X_test_no_preg_normalized)

# Calculate and print the metrics
print("Original Dataset without Pregnancies Column - Normalization")
print(f"Accuracy: {accuracy_score(y_test, y_pred_no_preg_norm)}")
print(f"Precision: {precision_score(y_test, y_pred_no_preg_norm)}")
print(f"Recall: {recall_score(y_test, y_pred_no_preg_norm)}")


# Código:
# Standardize the features
X_train_no_preg_standardized = scaler_std.fit_transform(X_train_no_preg)
X_test_no_preg_standardized = scaler_std.transform(X_test_no_preg)
knn.fit(X_train_no_preg_standardized, y_train)
y_pred_no_preg_std = knn.predict(X_test_no_preg_standardized)

# Calculate and print the metrics
print("Original Dataset without Pregnancies Column - Standardization")
print(f"Accuracy: {accuracy_score(y_test, y_pred_no_preg_std)}")
print(f"Precision: {precision_score(y_test, y_pred_no_preg_std)}")
print(f"Recall: {recall_score(y_test, y_pred_no_preg_std)}")


# Texto:
## Conjunto de Datos Sin Outliers (Sin Cambios)

# Código:
# Split the data without outliers into features and target variable
X_no_outliers = df_no_outliers.drop('Outcome', axis=1)
y_no_outliers = df_no_outliers['Outcome']

# Split the data into training and testing sets
X_train_no_outliers, X_test_no_outliers, y_train_no_outliers, y_test_no_outliers = train_test_split(X_no_outliers, y_no_outliers, test_size=0.2, random_state=random_state_result)
knn.fit(X_train_no_outliers, y_train_no_outliers)
y_pred_no_outliers = knn.predict(X_test_no_outliers)

# Calculate and print metrics
print("Dataset No Outliers - Without Scaling")
print(f"Accuracy: {accuracy_score(y_test_no_outliers, y_pred_no_outliers)}")
print(f"Precision: {precision_score(y_test_no_outliers, y_pred_no_outliers)}")
print(f"Recall: {recall_score(y_test_no_outliers, y_pred_no_outliers)}")



# Código:

# Normalize the features
X_train_no_outliers_normalized = scaler_norm.fit_transform(X_train_no_outliers)
X_test_no_outliers_normalized = scaler_norm.transform(X_test_no_outliers)
knn.fit(X_train_no_outliers_normalized, y_train_no_outliers)
y_pred_no_outliers_norm = knn.predict(X_test_no_outliers_normalized)

# Calculate and print the metrics
print("Dataset No Outliers - Normalization")
print(f"Accuracy: {accuracy_score(y_test_no_outliers, y_pred_no_outliers_norm)}")
print(f"Precision: {precision_score(y_test_no_outliers, y_pred_no_outliers_norm)}")
print(f"Recall: {recall_score(y_test_no_outliers, y_pred_no_outliers_norm)}")


# Código:
# Standardize the features
X_train_no_outliers_standardized = scaler_std.fit_transform(X_train_no_outliers)
X_test_no_outliers_standardized = scaler_std.transform(X_test_no_outliers)
knn.fit(X_train_no_outliers_standardized, y_train_no_outliers)
y_pred_no_outliers_std = knn.predict(X_test_no_outliers_standardized)

# Calculate and print the metrics
print("Dataset No Outliers - Standardization")
print(f"Accuracy: {accuracy_score(y_test_no_outliers, y_pred_no_outliers_std)}")
print(f"Precision: {precision_score(y_test_no_outliers, y_pred_no_outliers_std)}")
print(f"Recall: {recall_score(y_test_no_outliers, y_pred_no_outliers_std)}")


# Texto:
## Caso 3: Conjunto de Datos Sin Outliers y Sin la Columna de Embarazos

# Código:
# Drop the 'Pregnancies' column from the dataset without outliers
X_no_outliers_no_preg = df_no_outliers.drop(['Outcome', 'Pregnancies'], axis=1)
X_train_no_outliers_no_preg, X_test_no_outliers_no_preg, y_train_no_outliers, y_test_no_outliers = train_test_split(X_no_outliers_no_preg, y_no_outliers, test_size=0.2, random_state=random_state_result)

knn.fit(X_train_no_outliers_no_preg, y_train_no_outliers)
y_pred_no_outliers_no_preg = knn.predict(X_test_no_outliers_no_preg)

# Calculate and print metrics
print("Dataset No Outliers without Pregnancies Column - Without Scaling")
print(f"Accuracy: {accuracy_score(y_test_no_outliers, y_pred_no_outliers_no_preg)}")
print(f"Precision: {precision_score(y_test_no_outliers, y_pred_no_outliers_no_preg)}")
print(f"Recall: {recall_score(y_test_no_outliers, y_pred_no_outliers_no_preg)}")


# Código:
# Normalize the features
X_train_no_outliers_no_preg_normalized = scaler_norm.fit_transform(X_train_no_outliers_no_preg)
X_test_no_outliers_no_preg_normalized = scaler_norm.transform(X_test_no_outliers_no_preg)

knn.fit(X_train_no_outliers_no_preg_normalized, y_train_no_outliers)
y_pred_no_outliers_no_preg_norm = knn.predict(X_test_no_outliers_no_preg_normalized)

# Calculate and print the metrics
print("Dataset No Outliers without Pregnancies Column - Normalization")
print(f"Accuracy: {accuracy_score(y_test_no_outliers, y_pred_no_outliers_no_preg_norm)}")
print(f"Precision: {precision_score(y_test_no_outliers, y_pred_no_outliers_no_preg_norm)}")
print(f"Recall: {recall_score(y_test_no_outliers, y_pred_no_outliers_no_preg_norm)}")


# Código:
# Standardize the features
X_train_no_outliers_no_preg_standardized = scaler_std.fit_transform(X_train_no_outliers_no_preg)
X_test_no_outliers_no_preg_standardized = scaler_std.transform(X_test_no_outliers_no_preg)

knn.fit(X_train_no_outliers_no_preg_standardized, y_train_no_outliers)
y_pred_no_outliers_no_preg_std = knn.predict(X_test_no_outliers_no_preg_standardized)

# Calculate and print the metrics
print("Dataset No Outliers without Pregnancies Column - Standardization")
print(f"Accuracy: {accuracy_score(y_test_no_outliers, y_pred_no_outliers_no_preg_std)}")
print(f"Precision: {precision_score(y_test_no_outliers, y_pred_no_outliers_no_preg_std)}")
print(f"Recall: {recall_score(y_test_no_outliers, y_pred_no_outliers_no_preg_std)}")


# Texto:
## Oversampling con SMOTE

# Código:
from imblearn.over_sampling import SMOTE

# Apply SMOTE to the original dataset
smote = SMOTE(random_state=random_state_result)
X_smote, y_smote = smote.fit_resample(X, y)

X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.2, random_state=random_state_result)
# Train the KNN model
knn = KNeighborsClassifier()
knn.fit(X_train_smote, y_train_smote)
y_pred_smote = knn.predict(X_test_smote)

print("Oversampled Dataset - Without Scaling")
print(f"Accuracy: {accuracy_score(y_test_smote, y_pred_smote)}")
print(f"Precision: {precision_score(y_test_smote, y_pred_smote)}")
print(f"Recall: {recall_score(y_test_smote, y_pred_smote)}")

# Repeat normalization and standardization steps for the augmented dataset with SMOTE


# Código:

# Apply SMOTE to the dataset without outliers
smote = SMOTE(random_state=random_state_result)
X_no_outliers_smote, y_no_outliers_smote = smote.fit_resample(X_no_outliers, y_no_outliers)

X_train_no_outliers_smote, X_test_no_outliers_smote, y_train_no_outliers_smote, y_test_no_outliers_smote = train_test_split(X_no_outliers_smote, y_no_outliers_smote, test_size=0.2, random_state=random_state_result)

knn.fit(X_train_no_outliers_smote, y_train_no_outliers_smote)
y_pred_no_outliers_smote = knn.predict(X_test_no_outliers_smote)

print("Oversampled No Outliers Dataset - Without Scaling")
print(f"Accuracy: {accuracy_score(y_test_no_outliers_smote, y_pred_no_outliers_smote)}")
print(f"Precision: {precision_score(y_test_no_outliers_smote, y_pred_no_outliers_smote)}")
print(f"Recall: {recall_score(y_test_no_outliers_smote, y_pred_no_outliers_smote)}")


# Código:

smote = SMOTE(random_state=random_state_result)
X_no_outliers_smote, y_no_outliers_smote = smote.fit_resample(X_no_outliers, y_no_outliers)
X_train_no_outliers_smote, X_test_no_outliers_smote, y_train_no_outliers_smote, y_test_no_outliers_smote = train_test_split(X_no_outliers_smote, y_no_outliers_smote, test_size=0.2, random_state=random_state_result)

scaler_std = StandardScaler()
X_train_no_outliers_smote_standardized = scaler_std.fit_transform(X_train_no_outliers_smote)
X_test_no_outliers_smote_standardized = scaler_std.transform(X_test_no_outliers_smote)

knn.fit(X_train_no_outliers_smote_standardized, y_train_no_outliers_smote)
y_pred_no_outliers_smote_std = knn.predict(X_test_no_outliers_smote_standardized)

print("Oversampled No Outliers Dataset - Standardization")
print(f"Accuracy: {accuracy_score(y_test_no_outliers_smote, y_pred_no_outliers_smote_std)}")
print(f"Precision: {precision_score(y_test_no_outliers_smote, y_pred_no_outliers_smote_std)}")
print(f"Recall: {recall_score(y_test_no_outliers_smote, y_pred_no_outliers_smote_std)}")


# Texto:
## All test

# Código:
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt

# Function to plot confusion matrix
def plot_confusion_matrix(cm, title):
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(title)
    plt.xticks([0.5, 1.5], ['Non-diabetic', 'Diabetic'], ha='right')
    plt.yticks([0.5, 1.5], ['Non-diabetic', 'Diabetic'], va='center')
    plt.show()

# Function to plot ROC curve
def plot_roc_curve(fpr, tpr, roc_auc, title):
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.show()

# Define function to calculate weighted score
def weighted_score(accuracy, recall, precision):
    return (0.4 * accuracy) + (0.3 * recall) + (0.3 * precision)

# Scalers initialization
scaler_norm = MinMaxScaler()
scaler_std = StandardScaler()

# K values to test
k_values = [3, 5, 9]

# Tests to perform
tests = [
    ('Original Dataset', X, y),
    ('Orig Dataset no Preg', X_no_preg, y),
    ('Dataset No Outliers', X_no_outliers, y_no_outliers),
    ('Dataset No Outliers no Preg', X_no_outliers_no_preg, y_no_outliers),
    ('Oversampled Dataset', X_smote, y_smote)
]

# Dictionary to store the best tests based on weighted score and include metrics for confusion matrix and AUC
best_tests = {}

for k in k_values:
    for test_name, x_dataset, y_dataset in tests:
        # Splitting the dataset
        X_train, X_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=random_state_result)
        knn = KNeighborsClassifier(n_neighbors=k)

        # Loop through different data preprocessing techniques
        for scale_method, (X_train_scaled, X_test_scaled) in [('Without Scaling', (X_train, X_test)), 
                                                              ('Normalization', (scaler_norm.fit_transform(X_train), scaler_norm.transform(X_test))), 
                                                              ('Standardization', (scaler_std.fit_transform(X_train), scaler_std.transform(X_test)))]:
            knn.fit(X_train_scaled, y_train)
            y_pred = knn.predict(X_test_scaled)
            acc = accuracy_score(y_test, y_pred)
            prec = precision_score(y_test, y_pred)
            rec = recall_score(y_test, y_pred)
            cm = confusion_matrix(y_test, y_pred)
            fpr, tpr, _ = roc_curve(y_test, knn.predict_proba(X_test_scaled)[:, 1])
            roc_auc = auc(fpr, tpr)

            # Storing results
            test_key = f"{test_name:27} - K={k} - {scale_method:16}"
            best_tests[test_key] = {
                'weighted_score': weighted_score(acc, rec, prec),
                'confusion_matrix': cm,
                'fpr': fpr, 
                'tpr': tpr,
                'roc_auc': roc_auc
            }
            print(f"{test_key}: Accuracy={acc:.2f}, Precision={prec:.2f}, Recall={rec:.2f} Weighted Score={best_tests[test_key]['weighted_score']:.2f}, AUC={roc_auc:.2f}")

# Identify the three best tests based on the weighted score
top_3_tests = sorted(best_tests, key=lambda x: best_tests[x]['weighted_score'], reverse=True)[:3]
print("\nTop 3 tests based on weighted score:")
for test in top_3_tests:
    print(f"{test}: Weighted Score={best_tests[test]['weighted_score']:.2f}, AUC={best_tests[test]['roc_auc']:.2f}")

    # Plotting confusion matrix and ROC curve for the top 3 tests
    plot_confusion_matrix(best_tests[test]['confusion_matrix'], f"Confusion Matrix for {test}")
    plot_roc_curve(best_tests[test]['fpr'], best_tests[test]['tpr'], best_tests[test]['roc_auc'], f"ROC Curve for {test}")


